import { useEffect, useRef, useState } from 'react';
import * as faceapi from 'face-api.js';
import { Camera } from 'lucide-react';
import { checkInApi, findEmployeeByName } from "../api/attendanceAPI";


type StatusType = 'default' | 'success' | 'error' | 'loading';

interface LabeledImageConfig {
  label: string;
  images: string[];
}

const LABELED_IMAGES: LabeledImageConfig[] = [
  {
    label: 'Nguyen Manh Hieu',
    images: ['/faces/nguyenmanhhieu.png'],
    
  },
  {
    label: 'Nguyen Hoang Linh',
    images: ['/faces/nguyenhoanglinh.png'],
    
  },
  {
    label: 'Ly Anh Huy',
    images: ['/faces/huy.png'],
  },
];
async function sendAttendance(name: string) {
  const emp = await findEmployeeByName(name);

  if (!emp || !emp.id) {
    console.log("‚ùå Kh√¥ng t√¨m th·∫•y nh√¢n vi√™n trong DB:", name);
    return;
  }

  const result = await checkInApi({
    employee_id: emp.id,
    name
  });

  if (result.success) {
    console.log("‚úÖ L∆∞u ƒëi·ªÉm danh th√†nh c√¥ng");
  } else {
    console.log("‚ùå L·ªói l∆∞u ƒëi·ªÉm danh:", result.error);
  }
}


export default function FaceAttendance() {
  const videoRef = useRef<HTMLVideoElement | null>(null);
  const canvasRef = useRef<HTMLCanvasElement | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const intervalRef = useRef<number | null>(null);
  const loginLockedRef = useRef(false);
  const [status, setStatus] = useState<string>('üü° ƒêang t·∫£i model...');
  const [statusType, setStatusType] = useState<StatusType>('loading');
  const [wrapperActive, setWrapperActive] = useState(false);

  useEffect(() => {
    let isCancelled = false;

    const loadModels = async () => {
      try {
        setStatus('üîÑ ƒêang t·∫£i model...');
        setStatusType('loading');

        // ‚ö†Ô∏è Path model: public/models/...
        await Promise.all([
          faceapi.nets.tinyFaceDetector.loadFromUri('/models/tiny_face_detector'),
          faceapi.nets.ssdMobilenetv1.loadFromUri('/models/ssd_mobilenetv1'),
          faceapi.nets.faceLandmark68Net.loadFromUri('/models/face_landmark_68'),
          faceapi.nets.faceRecognitionNet.loadFromUri('/models/face_recognition'),
        ]);

        if (isCancelled) return;

        console.log('‚úÖ Model ƒë√£ t·∫£i xong!');
        await startVideo();
      } catch (error) {
        console.error('‚ùå L·ªói t·∫£i model:', error);
        if (!isCancelled) {
          setStatus('‚ùå L·ªói khi t·∫£i model. Ki·ªÉm tra l·∫°i th∆∞ m·ª•c /public/models');
          setStatusType('error');
        }
      }
    };

    const startVideo = async () => {
      try {
        setStatus('üì∑ ƒêang m·ªü camera...');
        setStatusType('loading');

        const stream = await navigator.mediaDevices.getUserMedia({
          video: { width: 900, height: 650 },
        });

        if (isCancelled) {
          stream.getTracks().forEach((t) => t.stop());
          return;
        }

        streamRef.current = stream;

        if (!videoRef.current) return;
        videoRef.current.srcObject = stream;

        videoRef.current.onloadedmetadata = () => {
          console.log('üé• Camera s·∫µn s√†ng, b·∫Øt ƒë·∫ßu nh·∫≠n di·ªán...');
          videoRef.current?.play();
          void initRecognition();
        };
      } catch (error) {
        console.error('‚ùå Kh√¥ng m·ªü ƒë∆∞·ª£c camera:', error);
        if (!isCancelled) {
          setStatus('‚ùå Kh√¥ng th·ªÉ truy c·∫≠p camera!');
          setStatusType('error');
        }
      }
    };

    const loadLabeledImages = async () => {
      const labeledDescriptors: faceapi.LabeledFaceDescriptors[] = [];

      for (const person of LABELED_IMAGES) {
        const descriptors: Float32Array[] = [];

        for (const imgPath of person.images) {
          try {
            const img = await faceapi.fetchImage(imgPath);
            const det = await faceapi
              .detectSingleFace(img)
              .withFaceLandmarks()
              .withFaceDescriptor();
            if (det?.descriptor) {
              descriptors.push(det.descriptor);
            }
          } catch (error) {
            console.error('L·ªói load ·∫£nh m·∫´u:', imgPath, error);
          }
        }

        if (descriptors.length > 0) {
          labeledDescriptors.push(new faceapi.LabeledFaceDescriptors(person.label, descriptors));
        }
      }

      return labeledDescriptors;
    };

    const initRecognition = async () => {
      if (!videoRef.current || !canvasRef.current) return;

      setStatus('üü¢ Camera ƒë√£ b·∫≠t, ƒëang nh·∫≠n di·ªán...');
      setStatusType('default');

      const labeledDescriptors = await loadLabeledImages();
      if (labeledDescriptors.length === 0) {
        setStatus('‚ùå Kh√¥ng t·∫£i ƒë∆∞·ª£c ·∫£nh m·∫´u. Ki·ªÉm tra l·∫°i th∆∞ m·ª•c /public/known ho·∫∑c /public/faces.');
        setStatusType('error');
        return;
      }

      const matcher = new faceapi.FaceMatcher(labeledDescriptors, 0.4);

      const video = videoRef.current;
      const canvas = canvasRef.current;
      const ctx = canvas.getContext('2d');

      if (!ctx) return;

      const displaySize = {
        width: video.videoWidth || 900,
        height: video.videoHeight || 650,
      };

      canvas.width = displaySize.width;
      canvas.height = displaySize.height;
      faceapi.matchDimensions(canvas, displaySize);

      if (intervalRef.current) {
        window.clearInterval(intervalRef.current);
      }

      intervalRef.current = window.setInterval(async () => {
        if (!videoRef.current || !canvasRef.current) return;
        if (video.readyState !== 4) return;

        try {
          const detections = await faceapi
            .detectAllFaces(video, new faceapi.SsdMobilenetv1Options({ minConfidence: 0.5 }))
            .withFaceLandmarks()
            .withFaceDescriptors();

          ctx.clearRect(0, 0, canvas.width, canvas.height);

          const resized = faceapi.resizeResults(detections, displaySize);

          if (resized.length === 0) {
            setWrapperActive(false);
            if (!loginLockedRef.current) {
              setStatus('‚è≥ Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t...');
              setStatusType('default');
            }
            return;
          }

          resized.forEach((det) => {
            const best = matcher.findBestMatch(det.descriptor);
            const box = det.detection.box;

            const similarity = (1 - best.distance) * 100;
            const percentText = similarity.toFixed(1);
            const isUnknown = best.label === 'unknown';
            const color = isUnknown ? 'red' : '#00c853';

            // üéØ V·∫Ω khung glow, mirror gi·ªëng code c≈©
            ctx.save();
            ctx.scale(-1, 1);
            ctx.translate(-canvas.width, 0);
            ctx.shadowBlur = 15;
            ctx.shadowColor = color;
            ctx.strokeStyle = color;
            ctx.lineWidth = 3;
            ctx.strokeRect(
              canvas.width - (box.x + box.width),
              box.y,
              box.width,
              box.height
            );
            ctx.fillStyle = color;
            ctx.font = '18px Segoe UI';
            ctx.fillText(
              `${best.label} (${percentText}%)`,
              canvas.width - (box.x + box.width),
              box.y - 10
            );
            ctx.restore();

            // ‚úÖ Khi kh·ªõp >= 50% & kh√¥ng unknown & ch∆∞a lock
            const isMatch = !isUnknown && similarity >= 50;

            if (isMatch && !loginLockedRef.current) {
              loginLockedRef.current = true;
              setWrapperActive(true);
              const time = new Date().toLocaleTimeString();
              setStatus(`‚úÖ ${best.label} ƒë√£ ƒëƒÉng nh·∫≠p l√∫c ${time}`);
              // G·ª≠i d·ªØ li·ªáu ƒëi·ªÉm danh l√™n server l∆∞u v√†o MySQL
              setStatusType('success');
              // üëâ G·ªåI L∆ØU ƒêI·ªÇM DANH
              sendAttendance(best.label);
          
              setTimeout(() => {
                loginLockedRef.current = false;
                setWrapperActive(false);
                setStatus('üü¢ H·ªá th·ªëng s·∫µn s√†ng cho l∆∞·ª£t ti·∫øp theo...');
                setStatusType('default');
              }, 5000);

              
            } else if (!loginLockedRef.current) {
              setWrapperActive(false);
              setStatus('‚è≥ ƒêang x√°c th·ª±c...');
              setStatusType('loading');
            }
          });
        } catch (error) {
          console.error('L·ªói khi nh·∫≠n di·ªán:', error);
          if (!loginLockedRef.current) {
            setStatus('‚ùå L·ªói trong qu√° tr√¨nh nh·∫≠n di·ªán. Ki·ªÉm tra l·∫°i camera / model.');
            setStatusType('error');
          }
        }
      }, 300);
    };

    void loadModels();

    return () => {
      isCancelled = true;
      if (intervalRef.current) {
        window.clearInterval(intervalRef.current);
      }
      if (streamRef.current) {
        streamRef.current.getTracks().forEach((track) => track.stop());
      }
    };
  }, []);

  // Map statusType -> class Tailwind
  const statusClass = (() => {
    switch (statusType) {
      case 'success':
        return 'text-green-700 bg-green-50 shadow-[0_0_15px_rgba(34,197,94,0.3)]';
      case 'error':
        return 'text-red-700 bg-red-50 shadow-[0_0_15px_rgba(239,68,68,0.3)]';
      case 'loading':
        return 'text-amber-700 bg-amber-50 animate-pulse';
      default:
        return 'text-teal-700 bg-teal-50';
    }
  })();

  return (
    <div className="h-full w-full space-y-6">
      <div>
        <h1 className="flex items-center gap-2 text-2xl font-bold text-slate-800">
          <Camera className="h-7 w-7 text-emerald-500" />
          H·ªÜ TH·ªêNG CH·∫§M C√îNG B·∫∞NG KHU√îN M·∫∂T
        </h1>
        <p className="mt-1 text-sm text-slate-500">
          S·ª≠ d·ª•ng nh·∫≠n di·ªán khu√¥n m·∫∑t (tiny + SSD MobilenetV1) ƒë·ªÉ ch·∫•m c√¥ng cho nh√¢n vi√™n.
        </p>
      </div>

      <div className="flex flex-col items-center gap-4">
        {/* V√πng video ch√≠nh */}
        <div
          className={[
            'relative h-[650px] w-[900px] overflow-hidden rounded-2xl bg-white transition-all',
            'shadow-[0_0_18px_rgba(0,0,0,0.2)]',
            wrapperActive ? 'scale-[1.01] shadow-[0_0_40px_#00ff99]' : '',
          ].join(' ')}
        >
          <video
            ref={videoRef}
            autoPlay
            muted
            className="h-full w-full object-cover"
            style={{ transform: 'scaleX(-1)' }} // mirror nh∆∞ b·∫£n c≈©
          />
          <canvas
            ref={canvasRef}
            className="pointer-events-none absolute left-0 top-0 h-full w-full"
            style={{ transform: 'scaleX(-1)' }}
          />
        </div>

        {/* Th√¥ng b√°o tr·∫°ng th√°i */}
        <div
          className={[
            'mt-3 rounded-xl px-6 py-3 text-center text-base font-semibold shadow-sm transition-all',
            statusClass,
          ].join(' ')}
        >
          {status}
        </div>
      </div>
    </div>
  );
}
